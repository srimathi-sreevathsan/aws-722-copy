{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark data mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('readin').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the data. Note that it's in the csv\n",
    "\n",
    "#City,Date,PM2.5,PM10,NO,NO2,NOx,NH3,CO,SO2,O3,Benzene,Toluene,Xylene,AQI,AQI_Bucket\n",
    "#define the schema\n",
    "\n",
    "# Let's import in the relevant types.\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql.types import *\n",
    "Schema=StructType([\n",
    "  StructField(\"City\",StringType(),nullable=True),\n",
    "  StructField(\"Date\",StringType(),nullable=True),\n",
    "  StructField(\"PM25\",FloatType(),nullable=True),\n",
    "  StructField(\"PM10\",FloatType(),nullable=True),\n",
    "  StructField(\"NO\",FloatType(),nullable=True),\n",
    "  StructField(\"NO2\",FloatType(),nullable=True),\n",
    "  StructField(\"NOX\",FloatType(),nullable=True),\n",
    "  StructField(\"NH3\",FloatType(),nullable=True),\n",
    "  StructField(\"CO\",FloatType(),nullable=True),\n",
    "  StructField(\"SO2\",FloatType(),nullable=True),\n",
    "  StructField(\"O3\",FloatType(),nullable=True),\n",
    "  StructField(\"benzene\",FloatType(),nullable=True),\n",
    "  StructField(\"toluene\",FloatType(),nullable=True),\n",
    "  StructField(\"Xylene\",FloatType(),nullable=True),\n",
    "  StructField(\"AQI\",FloatType(),nullable=True),\n",
    "  StructField(\"AQIBucket\",StringType(),nullable=True)\n",
    "])\n",
    "df = spark.read.option(\"header\",True).schema(Schema).csv(\"Datasets/city_day.csv\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+\n",
      "|     City|      Date|PM25|PM10|    NO|  NO2|   NOX| NH3|    CO|  SO2|    O3|benzene|toluene|Xylene| AQI|AQIBucket|\n",
      "+---------+----------+----+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+\n",
      "|Ahmedabad|2015-01-01|null|null|  0.92|18.22| 17.15|null|  0.92|27.64|133.36|    0.0|   0.02|   0.0|null|     null|\n",
      "|Ahmedabad|2015-01-02|null|null|  0.97|15.69| 16.46|null|  0.97|24.55| 34.06|   3.68|    5.5|  3.77|null|     null|\n",
      "|Ahmedabad|2015-01-03|null|null|  17.4| 19.3|  29.7|null|  17.4|29.07|  30.7|    6.8|   16.4|  2.25|null|     null|\n",
      "|Ahmedabad|2015-01-04|null|null|   1.7|18.48| 17.97|null|   1.7|18.59| 36.08|   4.43|  10.14|   1.0|null|     null|\n",
      "|Ahmedabad|2015-01-05|null|null|  22.1|21.42| 37.76|null|  22.1|39.33| 39.31|   7.01|  18.89|  2.78|null|     null|\n",
      "|Ahmedabad|2015-01-06|null|null| 45.41|38.48|  81.5|null| 45.41|45.76| 46.51|   5.42|  10.83|  1.93|null|     null|\n",
      "|Ahmedabad|2015-01-07|null|null|112.16|40.62|130.77|null|112.16|32.28| 33.47|    0.0|    0.0|   0.0|null|     null|\n",
      "|Ahmedabad|2015-01-08|null|null| 80.87|36.74| 96.75|null| 80.87|38.54| 31.89|    0.0|    0.0|   0.0|null|     null|\n",
      "|Ahmedabad|2015-01-09|null|null| 29.16| 31.0|  48.0|null| 29.16|58.68| 25.75|    0.0|    0.0|   0.0|null|     null|\n",
      "|Ahmedabad|2015-01-10|null|null|  null| 7.04|   0.0|null|  null| 8.29|  4.55|    0.0|    0.0|   0.0|null|     null|\n",
      "|Ahmedabad|2015-01-11|null|null|132.07| 55.8| 24.53|null|132.07|25.03|  6.79|    0.0|    0.0|   0.0|null|     null|\n",
      "|Ahmedabad|2015-01-12|null|null| 52.04|40.67| 90.24|null| 52.04|51.84| 45.89|   2.41|   0.03|  7.88|null|     null|\n",
      "|Ahmedabad|2015-01-13|null|null| 48.82| 44.2| 87.09|null| 48.82|68.21| 35.16|   9.45|  13.35|  12.5|null|     null|\n",
      "|Ahmedabad|2015-01-14|null|null|  19.2|27.86| 33.05|null|  19.2|52.65| 20.96|   2.16|   2.26|  5.19|null|     null|\n",
      "|Ahmedabad|2015-01-15|null|null|   0.6|16.96|  16.6|null|   0.6|28.89| 47.63|   0.14|   0.04|  1.35|null|     null|\n",
      "|Ahmedabad|2015-01-16|null|null|  1.63|21.72| 22.86|null|  1.63|38.27| 46.03|   0.35|   0.05|  2.01|null|     null|\n",
      "|Ahmedabad|2015-01-17|null|null| 11.44|24.73| 34.75|null| 11.44| 49.5| 52.24|   0.68|    0.0|  3.27|null|     null|\n",
      "|Ahmedabad|2015-01-18|null|null|   6.1|25.77| 29.57|null|   6.1|48.43| 53.49|   0.74|   0.21|  2.75|null|     null|\n",
      "|Ahmedabad|2015-01-19|null|null|  2.51|26.88| 27.45|null|  2.51|50.03| 49.48|   0.26|   0.02|   2.8|null|     null|\n",
      "|Ahmedabad|2015-01-20|null|null|  7.92| 26.8|  32.4|null|  7.92|58.87| 56.37|   0.24|   0.01|  3.97|null|     null|\n",
      "+---------+----------+----+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>City</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOX</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>benzene</th>\n",
       "      <th>toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "      <th>AQIBucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>29531</td>\n",
       "      <td>29531</td>\n",
       "      <td>24933</td>\n",
       "      <td>18391</td>\n",
       "      <td>25949</td>\n",
       "      <td>25946</td>\n",
       "      <td>25346</td>\n",
       "      <td>19203</td>\n",
       "      <td>27472</td>\n",
       "      <td>25677</td>\n",
       "      <td>25509</td>\n",
       "      <td>23908</td>\n",
       "      <td>21490</td>\n",
       "      <td>11422</td>\n",
       "      <td>24850</td>\n",
       "      <td>24850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>67.45057795305125</td>\n",
       "      <td>118.12710294174458</td>\n",
       "      <td>17.574729666828148</td>\n",
       "      <td>28.560659052670943</td>\n",
       "      <td>32.3091233306691</td>\n",
       "      <td>23.483476022437166</td>\n",
       "      <td>2.248598209145695</td>\n",
       "      <td>14.531977259530647</td>\n",
       "      <td>34.49143047007627</td>\n",
       "      <td>3.280840303665094</td>\n",
       "      <td>8.700972087396305</td>\n",
       "      <td>3.0701278206492613</td>\n",
       "      <td>166.4635814889336</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>64.66144940408857</td>\n",
       "      <td>90.60510976469615</td>\n",
       "      <td>22.785846340154315</td>\n",
       "      <td>24.474745771469095</td>\n",
       "      <td>31.646010951236956</td>\n",
       "      <td>25.68427498752751</td>\n",
       "      <td>6.962884271836143</td>\n",
       "      <td>18.13377486834342</td>\n",
       "      <td>21.694928182946093</td>\n",
       "      <td>15.811136401685069</td>\n",
       "      <td>19.969163666257156</td>\n",
       "      <td>6.323247403424576</td>\n",
       "      <td>140.69658509414992</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>949.99</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>390.68</td>\n",
       "      <td>362.21</td>\n",
       "      <td>467.63</td>\n",
       "      <td>352.89</td>\n",
       "      <td>175.81</td>\n",
       "      <td>193.86</td>\n",
       "      <td>257.73</td>\n",
       "      <td>455.03</td>\n",
       "      <td>454.85</td>\n",
       "      <td>170.37</td>\n",
       "      <td>2049.0</td>\n",
       "      <td>Very Poor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary           City        Date               PM25                PM10  \\\n",
       "0   count          29531       29531              24933               18391   \n",
       "1    mean           None        None  67.45057795305125  118.12710294174458   \n",
       "2  stddev           None        None  64.66144940408857   90.60510976469615   \n",
       "3     min      Ahmedabad  2015-01-01               0.04                0.01   \n",
       "4     max  Visakhapatnam  2020-07-01             949.99              1000.0   \n",
       "\n",
       "                   NO                 NO2                 NOX  \\\n",
       "0               25949               25946               25346   \n",
       "1  17.574729666828148  28.560659052670943    32.3091233306691   \n",
       "2  22.785846340154315  24.474745771469095  31.646010951236956   \n",
       "3                0.02                0.01                 0.0   \n",
       "4              390.68              362.21              467.63   \n",
       "\n",
       "                  NH3                 CO                 SO2  \\\n",
       "0               19203              27472               25677   \n",
       "1  23.483476022437166  2.248598209145695  14.531977259530647   \n",
       "2   25.68427498752751  6.962884271836143   18.13377486834342   \n",
       "3                0.01                0.0                0.01   \n",
       "4              352.89             175.81              193.86   \n",
       "\n",
       "                   O3             benzene             toluene  \\\n",
       "0               25509               23908               21490   \n",
       "1   34.49143047007627   3.280840303665094   8.700972087396305   \n",
       "2  21.694928182946093  15.811136401685069  19.969163666257156   \n",
       "3                0.01                 0.0                 0.0   \n",
       "4              257.73              455.03              454.85   \n",
       "\n",
       "               Xylene                 AQI  AQIBucket  \n",
       "0               11422               24850      24850  \n",
       "1  3.0701278206492613   166.4635814889336       None  \n",
       "2   6.323247403424576  140.69658509414992       None  \n",
       "3                 0.0                13.0       Good  \n",
       "4              170.37              2049.0  Very Poor  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The show method allows you visualise DataFrames. We can see that there are two columns. \n",
    "df.show()\n",
    "\n",
    "# You could also try this. \n",
    "df.columns\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "df.describe().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the describe method get some general statistics on our data too. Remember to show the DataFrame!\n",
    "# But what about data type?\n",
    "# Then create a variable with the correct structure.\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For type, we can use print schema. \n",
    "# But wait! What if you want to change the format of the data? Maybe change age to an integer instead of long?\n",
    "# And now we can read in the data using that schema. If we print the schema, we can see that age is now an integer.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().toPandas()\n",
    "\n",
    "df.groupby('AQIBucket').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.groupby('City').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()\n",
    "# Let's see the data. You'll notice nulls.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "df.createOrReplaceTempView('pollution')\n",
    "\n",
    "# After that, we can use the SQL programming language for queries. \n",
    "results = spark.sql(\"SELECT * FROM pollution\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, we can use the SQL programming language for queries. \n",
    "results1 = spark.sql(\"SELECT city, count(City) FROM pollution where AQI is null group by City\")\n",
    "results1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find count for empty, None, Null, Nan with string literals.\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df2 = df.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "fig = plt.figure(figsize=(25,13))\n",
    "st = fig.suptitle(\"Distribution of features\",fontsize = 50, verticalalignment=\"center\")\n",
    "for col,num in zip(df.toPandas().describe().columns, range(1,11)):\n",
    "    ax = fig.add_subplot(3,4, num)\n",
    "    ax.hist(df.toPandas()[col])\n",
    "    plt.grid(False)\n",
    "    plt.xticks(rotation=45, fontsize=20)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.title(col.upper(), fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "st.set_y(0.95)\n",
    "fig.subplots_adjust(top=0.85, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required functions\n",
    "from pyspark.sql.functions import year \n",
    "df.createOrReplaceTempView('pollution')\n",
    "\n",
    "results1 = spark.sql(\"SELECT city, Count(AQIBucket), count(City) FROM pollution where AQIBucket is not null group by City, AQIBucket\")\n",
    "results1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires a certain amount of non-null values. Row two was dropped, as there's only one non-null value.\n",
    "df.na.drop(thresh=8).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import first,last\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = first(df['PM25'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = df.withColumn('PM25', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) \n",
    "spark_df_filled.show()\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(-sys.maxsize,0)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = last(spark_df_filled['PM25'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = spark_df_filled.withColumn('PM25', filled_column)\n",
    "# show off our glorious achievements\n",
    "\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) \n",
    "spark_df_filled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled.filter(spark_df_filled.PM25.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find count for empty, None, Null, Nan with string literals.\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df2 = spark_df_filled\n",
    "df2 = df2.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df2.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last, first\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(-sys.maxsize , 0)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = last(spark_df_filled['PM10'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = spark_df_filled.withColumn('PM10', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) \n",
    "spark_df_filled.show()\n",
    "\n",
    "\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column= first(spark_df_filled['PM10'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = spark_df_filled.withColumn('PM10', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) \n",
    "spark_df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "\n",
    "dff= spark_df_filled\n",
    "dff.createOrReplaceTempView('pollution')\n",
    "\n",
    "results = spark.sql(\"SELECT * FROM pollution where pm10 is null\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.filter(dff.PM10.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "mean_pm10 = dff.select(mean(dff['PM10'])).collect()\n",
    "mean_pm10[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dff1 = dff.na.fill(mean_pm10[0][0], subset=['PM10'])\n",
    "dff1.filter(dff1.PM10.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dff.filter(dff.PM25.isNull()).show()\n",
    "#from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "#dff.select([count(when(isNull(c), c)).alias(c) for c in dff.columns]).show()\n",
    "\n",
    "# Find count for empty, None, Null, Nan with string literals.\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "dffs = dff1.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in dff.columns])\n",
    "dffs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Get Year from date or Time column\n",
    "spark_df_filled = dff1\n",
    "spark_df_filled1 = spark_df_filled.withColumn(\"year\",year(\"Date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2= spark_df_filled1\n",
    "dff2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2.createOrReplaceTempView('pollution')\n",
    "\n",
    "results = spark.sql(\"SELECT * FROM pollution where PM10 is null\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2.count()\n",
    "dff2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.show() dff5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect the average. You'll notice that the collection returns the average in an interesting format.\n",
    "mean_no2 = dff2.select(mean(dff2['NO2'])).collect()\n",
    "mean_so2 = dff2.select(mean(dff2['SO2'])).collect()\n",
    "mean_o3 = dff2.select(mean(dff2['O3'])).collect()\n",
    "#mean_pm10\n",
    "mean_no2[0][0]\n",
    "dff3 = dff2.na.fill(mean_no2[0][0], subset=['NO2'])\n",
    "mean_so2[0][0]\n",
    "dff4 = dff3.na.fill(mean_so2[0][0], subset=['SO2'])\n",
    "mean_o3[0][0]\n",
    "dff5 = dff4.na.fill(mean_o3[0][0], subset=['O3'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df1=df1.drop('date')\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "dff6 = dff5.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in dff5.columns])\n",
    "dff6.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dff5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect the average. You'll notice that the collection returns the average in an interesting format.\n",
    "#mean_pm10 = df.select(mean(df['PM10'])).collect()\n",
    "#mean_pm10\n",
    "#mean_pm10[0][0]\n",
    "#df.filter(df.PM10.isNull()).show()\n",
    "#df1 = df1.na.fill(mean_pm10[0][0], subset=['PM10'])\n",
    "#df1.filter(df1.PM10.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dftr=dff5.drop('NO')\n",
    "dftr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dftr=dftr.drop('NOX')\n",
    "dftr=dftr.drop('NH3')\n",
    "dftr=dftr.drop('CO')\n",
    "dftr=dftr.drop('benzene')\n",
    "dftr=dftr.drop('toluene')\n",
    "dftr=dftr.drop('Xylene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr.show()\n",
    "#df1=df1.drop('date')\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "dftr1 = dftr.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in dftr.columns])\n",
    "dftr1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = df1.na.fill(mean_pm10[0][0], subset=['PM10'])\n",
    "#dftr2.filter(dftr.AQI.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 0 for null on only population column \n",
    "#df2 = df2.na.fill(value=0,subset=[\"AQI\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "dftr.withColumn(\"AQIBucket1\", func.last('AQIBucket', True).over(Window.partitionBy('City').orderBy('year').rowsBetween(-sys.maxsize, 0))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = first(dftr['AQIBucket'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = dftr.withColumn('AQIBucket', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled.createOrReplaceTempView('pollution')\n",
    "\n",
    "results2 = spark.sql(\"SELECT * FROM pollution where AQIBucket is null\")\n",
    "results2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(-sys.maxsize ,0 )\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = last(spark_df_filled['AQIBucket'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled2 = spark_df_filled.withColumn('AQIBucket', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled2.orderBy('City', 'Date').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled2.createOrReplaceTempView('pollution')\n",
    "\n",
    "results3 = spark.sql(\"SELECT * FROM pollution where AQIBucket is null\")\n",
    "results3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled2.createOrReplaceTempView('pollution')\n",
    "\n",
    "results4 = spark.sql(\"SELECT AQIBucket, count(AQIBucket),city,year FROM pollution group by AQIbucket,city,year\")\n",
    "results4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###check the AQI for the rows, and update the nulls\n",
    "\n",
    "spark_df_filled2.filter(spark_df_filled2.AQI.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('AQIBucket')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = first(spark_df_filled2['AQI'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled3 = spark_df_filled2.withColumn('AQII', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled3.orderBy('City', 'AQII').show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark_df_filled3.filter(spark_df_filled3.AQII.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled3.createOrReplaceTempView('pollution')\n",
    "\n",
    "results4 = spark.sql(\"SELECT count(AQIBucket),AQIBucket,city,year FROM pollution group by AQIbucket,city,year\")\n",
    "results4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transform with String Indexer and OneHotEncoder\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"AQIBucket\", outputCol=\"AQIB_index\").fit(spark_df_filled3)\n",
    "spark_df_ind = indexer.transform(spark_df_filled3)\n",
    "spark_df_ind.show()\n",
    "\n",
    "\n",
    "#df3 = encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_ind.createOrReplaceTempView('pollution')\n",
    "\n",
    "results4 = spark.sql(\"SELECT AQIBucket ,AQIB_index, count(AQIB_index), city,year FROM pollution group by AQIBucket, AQIB_index, city,year\")\n",
    "results4.show()\n",
    "result = spark.sql(\"Select * from pollution\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###one hot encoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"AQIB_index\", outputCol=\"AQIB_vec\")\n",
    "ohe = encoder.fit(spark_df_ind) # indexer is the existing dataframe, see the question\n",
    "encoded = ohe.transform(spark_df_ind)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark_df_filled3.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = ['AQIBucket']\n",
    "numCols = ['PM25','PM10','NO2','SO2','O3']\n",
    "numcolout = ['AQII']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numCols)\n",
    "print(catCols)\n",
    "print(numcolout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "va = VectorAssembler(inputCols=numCols, outputCol=\"SS_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = va.transform(spark_df_filled3)\n",
    "\n",
    "temp_train.show(2, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train.select(\"SS_features\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=StandardScaler(inputCol=\"SS_features\", outputCol=\"Scaled\")\n",
    "train1 = ss.fit(temp_train).transform(temp_train)\n",
    "train1.select(\"Scaled\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "mms = MinMaxScaler(inputCol=\"SS_features\", outputCol=\"MMScaled\")\n",
    "train = mms.fit(temp_train).transform(temp_train)\n",
    "train.select(\"MMScaled\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "catCols = ['AQIBucket']\n",
    "numCols = ['PM25','PM10','NO2','SO2','O3']\n",
    "numcolout = ['AQII']\n",
    "\n",
    "# The input columns are the feature column names, and the output column is what you'd like the new column to be named. \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"PM25\",\"NO2\",\"SO2\",\"O3\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# Now that we've created the assembler variable, let's actually transform the data.\n",
    "output = assembler.transform(spark_df_filled3)\n",
    "\n",
    "\n",
    "# Using print schema, you see that the features output column has been added. \n",
    "output.printSchema()\n",
    "\n",
    "# You can see that the features column is a dense vector that combines the various features as expected.\n",
    "output.head()\n",
    "\n",
    "output.select(['features']).toPandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select two columns (the feature and predictor).\n",
    "# This is now in the appropriate format to be processed by Spark.\n",
    "final_data = output.select(\"features\",'AQII')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a randomised 70/30 split. \n",
    "# Remember, you can use other splits depending on how easy/difficult it is to train your model.\n",
    "train_data,test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see our training data.\n",
    "train_data.describe().show()\n",
    "\n",
    "# And our testing data.\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import LinearRegression library\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol='AQII')\n",
    "# Fit the model to the data.\n",
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linear regression.\n",
    "print(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the model against the test data.\n",
    "test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting results! This shows the difference between the predicted value and the test data.\n",
    "test_results.residuals.show()\n",
    "\n",
    "# Let's get some evaluation metrics (as discussed in the previous linear regression notebook).\n",
    "print(\"RSME: {}\".format(test_results.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the R2 value. \n",
    "print(\"R2: {}\".format(test_results.r2))\n",
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transform with String Indexer and OneHotEncoder\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "indexer0 = StringIndexer(inputCol=\"City\", outputCol=\"City_index\").fit(spark_df_filled3)\n",
    "spark_df = indexer0.transform(spark_df_filled3)\n",
    "spark_df.show()\n",
    "\n",
    "###one hot encoder\n",
    "\n",
    "encoder0 = OneHotEncoder(inputCol=\"City_index\", outputCol=\"City_vec\")\n",
    "ohe = encoder0.fit(spark_df) # indexer is the existing dataframe, see the question\n",
    "encoded0 = ohe.transform(spark_df)\n",
    "encoded0.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transform with String Indexer and OneHotEncoder\n",
    "\n",
    "indexer1 = StringIndexer(inputCol=\"AQIBucket\", outputCol=\"AQIB_index\").fit(encoded0)\n",
    "spark_df_ind1 = indexer1.transform(encoded0)\n",
    "spark_df_ind1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###one hot encoder\n",
    "\n",
    "encoder1 = OneHotEncoder(inputCol=\"AQIB_index\", outputCol=\"AQIB_vec\")\n",
    "ohe1 = encoder1.fit(spark_df_ind1) # indexer is the existing dataframe, see the question\n",
    "encoded1 = ohe1.transform(spark_df_ind1)\n",
    "encoded1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can assemble all of this as one vector in the features column. \n",
    "assembler = VectorAssembler(inputCols=['City_vec','PM25',\n",
    " 'PM10',\n",
    " 'NO2',\n",
    " 'SO2',\n",
    " 'O3'],outputCol='features')\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "# Note that survived is a categorial variable but didn't require any transformation.\n",
    "# That's because it's already in the format of 1's and 0's. \n",
    "log_reg = LogisticRegression(featuresCol='features',labelCol='AQIB_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists everything we want to do. Index data, encode data, assemble data and then pass in the actual model.\n",
    "pipeline = Pipeline(stages=[indexer0,indexer1,\n",
    "                           encoder0,encoder1,\n",
    "                           assembler,log_reg ])\n",
    "# Train/test split. \n",
    "train_data1, test_data1 = spark_df_filled3.randomSplit([0.7,.3])\n",
    "\n",
    "# Note pipeline. Call it as you would call a machine learning object.\n",
    "fit_model = pipeline.fit(train_data1)\n",
    "\n",
    "# Transform test data. \n",
    "results = fit_model.transform(test_data1)\n",
    "\n",
    "# Evaluate the model using the binary classifer.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='AQIB_index')\n",
    "\n",
    "# If we select the actual and predicted results, we can see that some predictions were correct while others were wrong.\n",
    "results.select('AQIB_index','prediction').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then evaluate using AUC (area under the curve). AUC is linked to ROC.\n",
    "AUC = my_eval.evaluate(results)\n",
    "\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant Python libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Convert sex to an array using Numpy and plot it using pyplot. \n",
    "sexArr = np.array(train1.select('AQIBucket').collect())\n",
    "plt.hist(sexArr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all interpretation has to be visualisations. You can also gain a lot of information with text.\n",
    "# For example, here we're seeing how much variance our model could account for.\n",
    "# According to this, the model got 164/209 correct. \n",
    "totalResults = results.select('AQIB_index','prediction')\n",
    "\n",
    "\n",
    "correctResults = totalResults.filter(totalResults['AQIB_INDEX'] == totalResults['prediction'])\n",
    "\n",
    "countTR = totalResults.count()\n",
    "print(\"Correct: \" + str(countTR))\n",
    "\n",
    "countTC = correctResults.count()\n",
    "print(\"Total Correct: \" + str(countTC)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "features = [\"PM25\",\"PM10\",\"NO2\",\"SO2\",\"O3\"]\n",
    "lr_data = spark_df_filled3.select(col(\"AQII\").alias(\"label\"), *features)\n",
    "lr_data.printSchema()\n",
    "\n",
    "# split the dataset into training and test\n",
    "(training, test) = lr_data.randomSplit([.7, .3], seed = 196)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the pipeline\n",
    "vectorAssembler = VectorAssembler(inputCols=features, outputCol=\"unscaled_features\")\n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "lr = LinearRegression(maxIter=10, regParam=.01)\n",
    "\n",
    "estimators = [vectorAssembler, standardScaler, lr]\n",
    "\n",
    "\n",
    "# building the model using the pipeline\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline\n",
    "pipeline = Pipeline(stages=estimators)\n",
    "\n",
    "model = pipeline.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "prediction = model.transform(test)\n",
    "\n",
    "prediction.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "\n",
    "rmse = eval.evaluate(prediction)\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "\n",
    "mse = eval.evaluate(prediction, {eval.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "\n",
    "mae = eval.evaluate(prediction, {eval.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# r2 - coefficient of determination\n",
    "\n",
    "r2 = eval.evaluate(prediction, {eval.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" %r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['PM25',\n",
    " 'PM10',\n",
    " 'NO2',\n",
    " 'SO2',\n",
    " 'O3','AQII'],outputCol=\"features\")\n",
    "\n",
    "# Let's transform the data. \n",
    "output = assembler.transform(spark_df_filled3)\n",
    "\n",
    "# Let's select the two columns we want. Features (which contains vectors), and the predictor.\n",
    "indexer = StringIndexer(inputCol=\"AQIBucket\", outputCol=\"AQIB_index\")\n",
    "output_fixed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fixed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###AQIB_index\n",
    "\n",
    "# Now we can assemble all of this as one vector in the features column. \n",
    "\n",
    "\n",
    "# Let's select the two columns we want. Features (which contains vectors), and the predictor.\n",
    "\n",
    "final_data = output_fixed.select('features','AQIB_index')\n",
    "\n",
    "# Split the training and testing set.\n",
    "train_data,test_data = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "# Let's import the relevant classifiers. \n",
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Use defaults to make the comparison \"fair\". This simplifies the comparison process.\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol='AQIB_index',featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='AQIB_index',featuresCol='features')\n",
    "gbt = GBTClassifier(labelCol='AQIB_index',featuresCol='features')\n",
    "\n",
    "# Train the models (it's three models, so it might take some time).\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "#gbt_model = gbt.fit(train_data)\n",
    "\n",
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "#gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start off with binary classification.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Note that the label column isn't named label, it's named PrivateIndex in this case.\n",
    "my_binary_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='AQIB_index')\n",
    "\n",
    "# This is the area under the curve. This indicates that the data is highly seperable.\n",
    "print(\"DTC\")\n",
    "print(my_binary_eval.evaluate(dtc_predictions))\n",
    "\n",
    "# RFC improves accuracy but also model complexity. RFC outperforms DTC in nearly every situation.\n",
    "print(\"RFC\")\n",
    "print(my_binary_eval.evaluate(rfc_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyrasterframes import TileExploder\n",
    "from pyrasterframes.rasterfunctions import *\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "exploder = TileExploder()\n",
    "assembler = VectorAssembler(inputCols=['PM25',\n",
    " 'PM10',\n",
    " 'NO2',\n",
    " 'SO2',\n",
    " 'O3','AQII'],outputCol=\"features\")\n",
    "\n",
    "kmeans = KMeans().setK(5).setFeaturesCol('features')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
