{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame - Basics\n",
    "\n",
    "Let's start off with the fundamentals of Spark DataFrame. \n",
    "\n",
    "Objective: In this exercise, you'll find out how to start a spark session, read in data, explore the data and manipuluate the data (using DataFrame syntax as well as SQL syntax). Let's get started! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('readin').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the data. Note that it's in the csv\n",
    "\n",
    "#City,Date,PM2.5,PM10,NO,NO2,NOx,NH3,CO,SO2,O3,Benzene,Toluene,Xylene,AQI,AQI_Bucket\n",
    "#define the schema\n",
    "\n",
    "# Let's import in the relevant types.\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql.types import *\n",
    "Schema=StructType([\n",
    "  StructField(\"City\",StringType(),nullable=True),\n",
    "  StructField(\"Date\",StringType(),nullable=True),\n",
    "  StructField(\"PM25\",FloatType(),nullable=True),\n",
    "  StructField(\"PM10\",FloatType(),nullable=True),\n",
    "  StructField(\"NO\",FloatType(),nullable=True),\n",
    "  StructField(\"NO2\",FloatType(),nullable=True),\n",
    "  StructField(\"NOX\",FloatType(),nullable=True),\n",
    "  StructField(\"NH3\",FloatType(),nullable=True),\n",
    "  StructField(\"CO\",FloatType(),nullable=True),\n",
    "  StructField(\"SO2\",FloatType(),nullable=True),\n",
    "  StructField(\"O3\",FloatType(),nullable=True),\n",
    "  StructField(\"benzene\",FloatType(),nullable=True),\n",
    "  StructField(\"toluene\",FloatType(),nullable=True),\n",
    "  StructField(\"Xylene\",FloatType(),nullable=True),\n",
    "  StructField(\"AQI\",FloatType(),nullable=True),\n",
    "  StructField(\"AQIBucket\",StringType(),nullable=True)\n",
    "])\n",
    "df = spark.read.option(\"header\",True).schema(Schema).csv(\"Datasets/city_day.csv\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The show method allows you visualise DataFrames. We can see that there are two columns. \n",
    "df.show()\n",
    "\n",
    "# You could also try this. \n",
    "df.columns\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "df.describe().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the describe method get some general statistics on our data too. Remember to show the DataFrame!\n",
    "# But what about data type?\n",
    "# Then create a variable with the correct structure.\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For type, we can use print schema. \n",
    "# But wait! What if you want to change the format of the data? Maybe change age to an integer instead of long?\n",
    "# And now we can read in the data using that schema. If we print the schema, we can see that age is now an integer.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().toPandas()\n",
    "\n",
    "df.groupby('AQIBucket').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.groupby('City').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()\n",
    "# Let's see the data. You'll notice nulls.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "df.createOrReplaceTempView('pollution')\n",
    "\n",
    "# After that, we can use the SQL programming language for queries. \n",
    "results = spark.sql(\"SELECT * FROM pollution\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, we can use the SQL programming language for queries. \n",
    "results1 = spark.sql(\"SELECT city, count(City) FROM pollution where AQI is null group by City\")\n",
    "results1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find count for empty, None, Null, Nan with string literals.\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df2 = df.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "fig = plt.figure(figsize=(25,13))\n",
    "st = fig.suptitle(\"Distribution of features\",fontsize = 50, verticalalignment=\"center\")\n",
    "for col,num in zip(df.toPandas().describe().columns, range(1,11)):\n",
    "    ax = fig.add_subplot(3,4, num)\n",
    "    ax.hist(df.toPandas()[col])\n",
    "    plt.grid(False)\n",
    "    plt.xticks(rotation=45, fontsize=20)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.title(col.upper(), fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "st.set_y(0.95)\n",
    "fig.subplots_adjust(top=0.85, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required functions\n",
    "from pyspark.sql.functions import year \n",
    "df.createOrReplaceTempView('pollution')\n",
    "\n",
    "results1 = spark.sql(\"SELECT city, Count(AQIBucket), count(City) FROM pollution where AQIBucket is not null group by City, AQIBucket\")\n",
    "results1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires a certain amount of non-null values. Row two was dropped, as there's only one non-null value.\n",
    "df.na.drop(thresh=8).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import first,last\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = first(df['PM25'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = df.withColumn('PM25', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) \n",
    "spark_df_filled.show()\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(-sys.maxsize,0)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = last(spark_df_filled['PM25'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = spark_df_filled.withColumn('PM25', filled_column)\n",
    "# show off our glorious achievements\n",
    "\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) \n",
    "spark_df_filled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled.filter(spark_df_filled.PM25.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find count for empty, None, Null, Nan with string literals.\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df2 = spark_df_filled\n",
    "df2 = df2.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df2.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last, first\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(-sys.maxsize , 0)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = last(spark_df_filled['PM10'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = spark_df_filled.withColumn('PM10', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) \n",
    "spark_df_filled.show()\n",
    "\n",
    "\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column= first(spark_df_filled['PM10'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = spark_df_filled.withColumn('PM10', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) \n",
    "spark_df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "\n",
    "dff= spark_df_filled\n",
    "dff.createOrReplaceTempView('pollution')\n",
    "\n",
    "results = spark.sql(\"SELECT * FROM pollution where pm10 is null\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.filter(dff.PM10.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "mean_pm10 = dff.select(mean(dff['PM10'])).collect()\n",
    "mean_pm10[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dff1 = dff.na.fill(mean_pm10[0][0], subset=['PM10'])\n",
    "dff1.filter(dff1.PM10.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dff.filter(dff.PM25.isNull()).show()\n",
    "#from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "#dff.select([count(when(isNull(c), c)).alias(c) for c in dff.columns]).show()\n",
    "\n",
    "# Find count for empty, None, Null, Nan with string literals.\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "dffs = dff1.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in dff.columns])\n",
    "dffs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Get Year from date or Time column\n",
    "spark_df_filled = dff1\n",
    "spark_df_filled1 = spark_df_filled.withColumn(\"year\",year(\"Date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2= spark_df_filled1\n",
    "dff2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2.createOrReplaceTempView('pollution')\n",
    "\n",
    "results = spark.sql(\"SELECT * FROM pollution where PM10 is null\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2.count()\n",
    "dff2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.show() dff5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect the average. You'll notice that the collection returns the average in an interesting format.\n",
    "mean_no2 = dff2.select(mean(dff2['NO2'])).collect()\n",
    "mean_so2 = dff2.select(mean(dff2['SO2'])).collect()\n",
    "mean_o3 = dff2.select(mean(dff2['O3'])).collect()\n",
    "#mean_pm10\n",
    "mean_no2[0][0]\n",
    "dff3 = dff2.na.fill(mean_no2[0][0], subset=['NO2'])\n",
    "mean_so2[0][0]\n",
    "dff4 = dff3.na.fill(mean_so2[0][0], subset=['SO2'])\n",
    "mean_o3[0][0]\n",
    "dff5 = dff4.na.fill(mean_o3[0][0], subset=['O3'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df1=df1.drop('date')\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "dff6 = dff5.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in dff5.columns])\n",
    "dff6.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dff5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect the average. You'll notice that the collection returns the average in an interesting format.\n",
    "#mean_pm10 = df.select(mean(df['PM10'])).collect()\n",
    "#mean_pm10\n",
    "#mean_pm10[0][0]\n",
    "#df.filter(df.PM10.isNull()).show()\n",
    "#df1 = df1.na.fill(mean_pm10[0][0], subset=['PM10'])\n",
    "#df1.filter(df1.PM10.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dftr=dff5.drop('NO')\n",
    "dftr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dftr=dftr.drop('NOX')\n",
    "dftr=dftr.drop('NH3')\n",
    "dftr=dftr.drop('CO')\n",
    "dftr=dftr.drop('benzene')\n",
    "dftr=dftr.drop('toluene')\n",
    "dftr=dftr.drop('Xylene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr.show()\n",
    "#df1=df1.drop('date')\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "dftr1 = dftr.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in dftr.columns])\n",
    "dftr1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = df1.na.fill(mean_pm10[0][0], subset=['PM10'])\n",
    "#dftr2.filter(dftr.AQI.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 0 for null on only population column \n",
    "#df2 = df2.na.fill(value=0,subset=[\"AQI\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "dftr.withColumn(\"AQIBucket1\", func.last('AQIBucket', True).over(Window.partitionBy('City').orderBy('year').rowsBetween(-sys.maxsize, 0))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = first(dftr['AQIBucket'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = dftr.withColumn('AQIBucket', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled.createOrReplaceTempView('pollution')\n",
    "\n",
    "results2 = spark.sql(\"SELECT * FROM pollution where AQIBucket is null\")\n",
    "results2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(-sys.maxsize ,0 )\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = last(spark_df_filled['AQIBucket'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled2 = spark_df_filled.withColumn('AQIBucket', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled2.orderBy('City', 'Date').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled2.createOrReplaceTempView('pollution')\n",
    "\n",
    "results3 = spark.sql(\"SELECT * FROM pollution where AQIBucket is null\")\n",
    "results3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled2.createOrReplaceTempView('pollution')\n",
    "\n",
    "results4 = spark.sql(\"SELECT AQIBucket, count(AQIBucket),city,year FROM pollution group by AQIbucket,city,year\")\n",
    "results4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###check the AQI for the rows, and update the nulls\n",
    "\n",
    "spark_df_filled2.filter(spark_df_filled2.AQI.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('AQIBucket')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = first(spark_df_filled2['AQI'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled3 = spark_df_filled2.withColumn('AQII', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled3.orderBy('City', 'AQII').show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark_df_filled3.filter(spark_df_filled3.AQII.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled3.createOrReplaceTempView('pollution')\n",
    "\n",
    "results4 = spark.sql(\"SELECT count(AQIBucket),AQIBucket,city,year FROM pollution group by AQIbucket,city,year\")\n",
    "results4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transform with String Indexer and OneHotEncoder\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"AQIBucket\", outputCol=\"AQIB_index\").fit(spark_df_filled3)\n",
    "spark_df_ind = indexer.transform(spark_df_filled3)\n",
    "spark_df_ind.show()\n",
    "\n",
    "\n",
    "#df3 = encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_ind.createOrReplaceTempView('pollution')\n",
    "\n",
    "results4 = spark.sql(\"SELECT AQIBucket ,AQIB_index, count(AQIB_index), city,year FROM pollution group by AQIBucket, AQIB_index, city,year\")\n",
    "results4.show()\n",
    "result = spark.sql(\"Select * from pollution\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###one hot encoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"AQIB_index\", outputCol=\"AQIB_vec\")\n",
    "ohe = encoder.fit(spark_df_ind) # indexer is the existing dataframe, see the question\n",
    "encoded = ohe.transform(spark_df_ind)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark_df_filled3.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = ['AQIBucket']\n",
    "numCols = ['PM25','PM10','NO2','SO2','O3','AQII']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numCols)\n",
    "print(catCols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "va = VectorAssembler(inputCols=numCols, outputCol=\"SS_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = va.transform(spark_df_filled3)\n",
    "\n",
    "temp_train.show(2, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train.select(\"SS_features\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=StandardScaler(inputCol=\"SS_features\", outputCol=\"Scaled\")\n",
    "train1 = ss.fit(temp_train).transform(train)\n",
    "train1.select(\"Scaled\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "mms = MinMaxScaler(inputCol=\"SS_features\", outputCol=\"MMScaled\")\n",
    "train = mms.fit(temp_train).transform(temp_train)\n",
    "train.select(\"MMScaled\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataframe columns \n",
    "encoded.columns\n",
    "\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "inputCols = [\n",
    " 'PM25',\n",
    " 'year',\n",
    " 'AQIB_index',\n",
    " 'AQIB_vec']\n",
    "\n",
    "outputCol = \"features\"\n",
    "df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "encoded = df_va.transform(encoded)\n",
    "encoded.select(['features']).toPandas().head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
