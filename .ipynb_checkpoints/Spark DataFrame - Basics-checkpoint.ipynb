{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame - Basics\n",
    "\n",
    "Let's start off with the fundamentals of Spark DataFrame. \n",
    "\n",
    "Objective: In this exercise, you'll find out how to start a spark session, read in data, explore the data and manipuluate the data (using DataFrame syntax as well as SQL syntax). Let's get started! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('readin').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the data. Note that it's in the csv\n",
    "\n",
    "#City,Date,PM2.5,PM10,NO,NO2,NOx,NH3,CO,SO2,O3,Benzene,Toluene,Xylene,AQI,AQI_Bucket\n",
    "#define the schema\n",
    "\n",
    "# Let's import in the relevant types.\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql.types import *\n",
    "Schema=StructType([\n",
    "  StructField(\"City\",StringType(),nullable=True),\n",
    "  StructField(\"Date\",StringType(),nullable=True),\n",
    "  StructField(\"PM25\",FloatType(),nullable=True),\n",
    "  StructField(\"PM10\",FloatType(),nullable=True),\n",
    "  StructField(\"NO\",FloatType(),nullable=True),\n",
    "  StructField(\"NO2\",FloatType(),nullable=True),\n",
    "  StructField(\"NOX\",FloatType(),nullable=True),\n",
    "  StructField(\"NH3\",FloatType(),nullable=True),\n",
    "  StructField(\"CO\",FloatType(),nullable=True),\n",
    "  StructField(\"SO2\",FloatType(),nullable=True),\n",
    "  StructField(\"O3\",FloatType(),nullable=True),\n",
    "  StructField(\"benzene\",FloatType(),nullable=True),\n",
    "  StructField(\"toluene\",FloatType(),nullable=True),\n",
    "  StructField(\"Xylene\",FloatType(),nullable=True),\n",
    "  StructField(\"AQI\",FloatType(),nullable=True),\n",
    "  StructField(\"AQIBucket\",StringType(),nullable=True)\n",
    "])\n",
    "df = spark.read.option(\"header\",True).schema(Schema).csv(\"Datasets/city_day.csv\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The show method allows you visualise DataFrames. We can see that there are two columns. \n",
    "df.show()\n",
    "\n",
    "# You could also try this. \n",
    "df.columns\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "df.describe().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the describe method get some general statistics on our data too. Remember to show the DataFrame!\n",
    "# But what about data type?\n",
    "# Then create a variable with the correct structure.\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For type, we can use print schema. \n",
    "# But wait! What if you want to change the format of the data? Maybe change age to an integer instead of long?\n",
    "# And now we can read in the data using that schema. If we print the schema, we can see that age is now an integer.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().toPandas()\n",
    "\n",
    "df.groupby('AQIBucket').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.groupby('City').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()\n",
    "# Let's see the data. You'll notice nulls.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "df.createOrReplaceTempView('pollution')\n",
    "\n",
    "# After that, we can use the SQL programming language for queries. \n",
    "results = spark.sql(\"SELECT * FROM pollution\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, we can use the SQL programming language for queries. \n",
    "results1 = spark.sql(\"SELECT city, count(City) FROM pollution where AQI is null group by City\")\n",
    "results1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "fig = plt.figure(figsize=(25,13))\n",
    "st = fig.suptitle(\"Distribution of features\",fontsize = 50, verticalalignment=\"center\")\n",
    "for col,num in zip(df.toPandas().describe().columns, range(1,11)):\n",
    "    ax = fig.add_subplot(3,4, num)\n",
    "    ax.hist(df.toPandas()[col])\n",
    "    plt.grid(False)\n",
    "    plt.xticks(rotation=45, fontsize=20)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.title(col.upper(), fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "st.set_y(0.95)\n",
    "fig.subplots_adjust(top=0.85, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required functions\n",
    "from pyspark.sql.functions import year \n",
    "df.createOrReplaceTempView('pollution')\n",
    "\n",
    "results1 = spark.sql(\"SELECT city, Count(AQIBucket), count(City) FROM pollution where AQIBucket is not null group by City, AQIBucket\")\n",
    "results1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires a certain amount of non-null values. Row two was dropped, as there's only one non-null value.\n",
    "df.na.drop(thresh=8).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, it's good practice to use your average to fill missing data. \n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Let's collect the average. You'll notice that the collection returns the average in an interesting format.\n",
    "mean_pm25 = df.select(mean(df['PM25'])).collect()\n",
    "#mean_pm25\n",
    "mean_pm25[0][0]\n",
    "\n",
    "# And finally, fill the missing values with the mean.\n",
    "#df.na.fill(mean_pm25, subset=['PM25']).show()\n",
    "#df.na.fill(mean_pm25[0][0], subset=['PM25']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "df.createOrReplaceTempView('pollution')\n",
    "\n",
    "results = spark.sql(\"SELECT * FROM pollution where pm25 is null\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.PM25.isNull()).show()\n",
    "df = df.na.fill(mean_pm25[0][0], subset=['PM25'])\n",
    "df.filter(df.PM25.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.PM25.isNull()).show()\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Get Year from date or Time column\n",
    "df = df.withColumn(\"year\",year(\"Date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect the average. You'll notice that the collection returns the average in an interesting format.\n",
    "mean_pm10 = df.select(mean(df['PM10'])).collect()\n",
    "#mean_pm10\n",
    "mean_pm10[0][0]\n",
    "df.filter(df.PM10.isNull()).show()\n",
    "df1 = df1.na.fill(mean_pm10[0][0], subset=['PM10'])\n",
    "df1.filter(df1.PM10.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.filter(df1.PM10.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect the average. You'll notice that the collection returns the average in an interesting format.\n",
    "mean_pm10 = df.select(mean(df['PM10'])).collect()\n",
    "#mean_pm10\n",
    "mean_pm10[0][0]\n",
    "df.filter(df.PM10.isNull()).show()\n",
    "df1 = df1.na.fill(mean_pm10[0][0], subset=['PM10'])\n",
    "df1.filter(df1.PM10.isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1=df1.drop('NO')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1=df1.drop('NOX')\n",
    "df1=df1.drop('NH3')\n",
    "df1=df1.drop('CO')\n",
    "df1=df1.drop('benzene')\n",
    "df1=df1.drop('toluene')\n",
    "df1=df1.drop('Xylene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 0 for null on only population column \n",
    "df2 = df1.na.fill(value=0,subset=[\"AQI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = df1.na.fill(mean_pm10[0][0], subset=['PM10'])\n",
    "df2.filter(df2.AQI.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 0 for null on only population column \n",
    "#df2 = df2.na.fill(value=0,subset=[\"AQI\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "df2.withColumn(\"AQIBucket1\", func.last('AQIBucket', True).over(Window.partitionBy('City').orderBy('year').rowsBetween(-sys.maxsize, 0))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = first(df['AQIBucket'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled = df.withColumn('AQIBucket', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled.orderBy('City', 'Date').show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled.createOrReplaceTempView('pollution')\n",
    "\n",
    "results2 = spark.sql(\"SELECT * FROM pollution where AQIBucket is null\")\n",
    "results2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/09 11:00:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, Date, PM2.5, PM10, NO, NO2, NOx, NH3, CO, SO2, O3, Benzene, Toluene, Xylene, AQI, AQI_Bucket\n",
      " Schema: City, Date, PM25, PM10, NO, NO2, NOX, NH3, CO, SO2, O3, benzene, toluene, Xylene, AQI, AQIBucket\n",
      "Expected: PM25 but found: PM2.5\n",
      "CSV file: file:///home/ubuntu/722/aws-722-copy/Datasets/city_day.csv\n",
      "[Stage 221:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+\n",
      "|     City|      Date|    PM25|PM10|    NO|  NO2|   NOX| NH3|    CO|  SO2|    O3|benzene|toluene|Xylene| AQI|AQIBucket|year|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+\n",
      "|Ahmedabad|2015-01-01|67.45058|null|  0.92|18.22| 17.15|null|  0.92|27.64|133.36|    0.0|   0.02|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-02|67.45058|null|  0.97|15.69| 16.46|null|  0.97|24.55| 34.06|   3.68|    5.5|  3.77|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-03|67.45058|null|  17.4| 19.3|  29.7|null|  17.4|29.07|  30.7|    6.8|   16.4|  2.25|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-04|67.45058|null|   1.7|18.48| 17.97|null|   1.7|18.59| 36.08|   4.43|  10.14|   1.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-05|67.45058|null|  22.1|21.42| 37.76|null|  22.1|39.33| 39.31|   7.01|  18.89|  2.78|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-06|67.45058|null| 45.41|38.48|  81.5|null| 45.41|45.76| 46.51|   5.42|  10.83|  1.93|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-07|67.45058|null|112.16|40.62|130.77|null|112.16|32.28| 33.47|    0.0|    0.0|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-08|67.45058|null| 80.87|36.74| 96.75|null| 80.87|38.54| 31.89|    0.0|    0.0|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-09|67.45058|null| 29.16| 31.0|  48.0|null| 29.16|58.68| 25.75|    0.0|    0.0|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-10|67.45058|null|  null| 7.04|   0.0|null|  null| 8.29|  4.55|    0.0|    0.0|   0.0|null|     Poor|2015|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "###backward fill\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('City')\\\n",
    "               .orderBy('Date')\\\n",
    "               .rowsBetween(-sys.maxsize ,0 )\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column = last(spark_df_filled['AQIBucket'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill\n",
    "spark_df_filled2 = spark_df_filled.withColumn('AQIBucket', filled_column)\n",
    "\n",
    "# show off our glorious achievements\n",
    "spark_df_filled2.orderBy('City', 'Date').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/09 11:00:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, Date, PM2.5, PM10, NO, NO2, NOx, NH3, CO, SO2, O3, Benzene, Toluene, Xylene, AQI, AQI_Bucket\n",
      " Schema: City, Date, PM25, PM10, NO, NO2, NOX, NH3, CO, SO2, O3, benzene, toluene, Xylene, AQI, AQIBucket\n",
      "Expected: PM25 but found: PM2.5\n",
      "CSV file: file:///home/ubuntu/722/aws-722-copy/Datasets/city_day.csv\n",
      "[Stage 224:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+---+---+---+---+---+---+---+-------+-------+------+---+---------+----+\n",
      "|City|Date|PM25|PM10| NO|NO2|NOX|NH3| CO|SO2| O3|benzene|toluene|Xylene|AQI|AQIBucket|year|\n",
      "+----+----+----+----+---+---+---+---+---+---+---+-------+-------+------+---+---------+----+\n",
      "+----+----+----+----+---+---+---+---+---+---+---+-------+-------+------+---+---------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled2.createOrReplaceTempView('pollution')\n",
    "\n",
    "results3 = spark.sql(\"SELECT * FROM pollution where AQIBucket is null\")\n",
    "results3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled2.createOrReplaceTempView('pollution')\n",
    "\n",
    "results4 = spark.sql(\"SELECT AQIBucket, count(AQIBucket),city,year FROM pollution group by AQIbucket,city,year\")\n",
    "results4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_filled2.createOrReplaceTempView('pollution')\n",
    "\n",
    "results4 = spark.sql(\"SELECT AQIBucket, count(AQIBucket),city,year FROM pollution where AQIBucket is null group by AQIbucket,city,year\")\n",
    "results4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/09 11:00:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, Date, PM2.5, PM10, NO, NO2, NOx, NH3, CO, SO2, O3, Benzene, Toluene, Xylene, AQI, AQI_Bucket\n",
      " Schema: City, Date, PM25, PM10, NO, NO2, NOX, NH3, CO, SO2, O3, benzene, toluene, Xylene, AQI, AQIBucket\n",
      "Expected: PM25 but found: PM2.5\n",
      "CSV file: file:///home/ubuntu/722/aws-722-copy/Datasets/city_day.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+\n",
      "|     City|      Date|    PM25|PM10|    NO|  NO2|   NOX| NH3|    CO|  SO2|    O3|benzene|toluene|Xylene| AQI|AQIBucket|year|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+\n",
      "|Ahmedabad|2015-01-01|67.45058|null|  0.92|18.22| 17.15|null|  0.92|27.64|133.36|    0.0|   0.02|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-02|67.45058|null|  0.97|15.69| 16.46|null|  0.97|24.55| 34.06|   3.68|    5.5|  3.77|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-03|67.45058|null|  17.4| 19.3|  29.7|null|  17.4|29.07|  30.7|    6.8|   16.4|  2.25|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-04|67.45058|null|   1.7|18.48| 17.97|null|   1.7|18.59| 36.08|   4.43|  10.14|   1.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-05|67.45058|null|  22.1|21.42| 37.76|null|  22.1|39.33| 39.31|   7.01|  18.89|  2.78|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-06|67.45058|null| 45.41|38.48|  81.5|null| 45.41|45.76| 46.51|   5.42|  10.83|  1.93|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-07|67.45058|null|112.16|40.62|130.77|null|112.16|32.28| 33.47|    0.0|    0.0|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-08|67.45058|null| 80.87|36.74| 96.75|null| 80.87|38.54| 31.89|    0.0|    0.0|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-09|67.45058|null| 29.16| 31.0|  48.0|null| 29.16|58.68| 25.75|    0.0|    0.0|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-10|67.45058|null|  null| 7.04|   0.0|null|  null| 8.29|  4.55|    0.0|    0.0|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-11|67.45058|null|132.07| 55.8| 24.53|null|132.07|25.03|  6.79|    0.0|    0.0|   0.0|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-12|67.45058|null| 52.04|40.67| 90.24|null| 52.04|51.84| 45.89|   2.41|   0.03|  7.88|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-13|67.45058|null| 48.82| 44.2| 87.09|null| 48.82|68.21| 35.16|   9.45|  13.35|  12.5|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-14|67.45058|null|  19.2|27.86| 33.05|null|  19.2|52.65| 20.96|   2.16|   2.26|  5.19|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-15|67.45058|null|   0.6|16.96|  16.6|null|   0.6|28.89| 47.63|   0.14|   0.04|  1.35|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-16|67.45058|null|  1.63|21.72| 22.86|null|  1.63|38.27| 46.03|   0.35|   0.05|  2.01|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-17|67.45058|null| 11.44|24.73| 34.75|null| 11.44| 49.5| 52.24|   0.68|    0.0|  3.27|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-18|67.45058|null|   6.1|25.77| 29.57|null|   6.1|48.43| 53.49|   0.74|   0.21|  2.75|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-19|67.45058|null|  2.51|26.88| 27.45|null|  2.51|50.03| 49.48|   0.26|   0.02|   2.8|null|     Poor|2015|\n",
      "|Ahmedabad|2015-01-20|67.45058|null|  7.92| 26.8|  32.4|null|  7.92|58.87| 56.37|   0.24|   0.01|  3.97|null|     Poor|2015|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_filled2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/09 11:00:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, Date, AQI_Bucket\n",
      " Schema: City, Date, AQIBucket\n",
      "Expected: AQIBucket but found: AQI_Bucket\n",
      "CSV file: file:///home/ubuntu/722/aws-722-copy/Datasets/city_day.csv\n",
      "22/10/09 11:00:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, Date, PM2.5, PM10, NO, NO2, NOx, NH3, CO, SO2, O3, Benzene, Toluene, Xylene, AQI, AQI_Bucket\n",
      " Schema: City, Date, PM25, PM10, NO, NO2, NOX, NH3, CO, SO2, O3, benzene, toluene, Xylene, AQI, AQIBucket\n",
      "Expected: PM25 but found: PM2.5\n",
      "CSV file: file:///home/ubuntu/722/aws-722-copy/Datasets/city_day.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+----------+\n",
      "|     City|      Date|    PM25|PM10|    NO|  NO2|   NOX| NH3|    CO|  SO2|    O3|benzene|toluene|Xylene| AQI|AQIBucket|year|AQIB_index|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+----------+\n",
      "|Ahmedabad|2015-01-01|67.45058|null|  0.92|18.22| 17.15|null|  0.92|27.64|133.36|    0.0|   0.02|   0.0|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-02|67.45058|null|  0.97|15.69| 16.46|null|  0.97|24.55| 34.06|   3.68|    5.5|  3.77|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-03|67.45058|null|  17.4| 19.3|  29.7|null|  17.4|29.07|  30.7|    6.8|   16.4|  2.25|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-04|67.45058|null|   1.7|18.48| 17.97|null|   1.7|18.59| 36.08|   4.43|  10.14|   1.0|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-05|67.45058|null|  22.1|21.42| 37.76|null|  22.1|39.33| 39.31|   7.01|  18.89|  2.78|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-06|67.45058|null| 45.41|38.48|  81.5|null| 45.41|45.76| 46.51|   5.42|  10.83|  1.93|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-07|67.45058|null|112.16|40.62|130.77|null|112.16|32.28| 33.47|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-08|67.45058|null| 80.87|36.74| 96.75|null| 80.87|38.54| 31.89|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-09|67.45058|null| 29.16| 31.0|  48.0|null| 29.16|58.68| 25.75|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-10|67.45058|null|  null| 7.04|   0.0|null|  null| 8.29|  4.55|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-11|67.45058|null|132.07| 55.8| 24.53|null|132.07|25.03|  6.79|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-12|67.45058|null| 52.04|40.67| 90.24|null| 52.04|51.84| 45.89|   2.41|   0.03|  7.88|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-13|67.45058|null| 48.82| 44.2| 87.09|null| 48.82|68.21| 35.16|   9.45|  13.35|  12.5|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-14|67.45058|null|  19.2|27.86| 33.05|null|  19.2|52.65| 20.96|   2.16|   2.26|  5.19|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-15|67.45058|null|   0.6|16.96|  16.6|null|   0.6|28.89| 47.63|   0.14|   0.04|  1.35|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-16|67.45058|null|  1.63|21.72| 22.86|null|  1.63|38.27| 46.03|   0.35|   0.05|  2.01|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-17|67.45058|null| 11.44|24.73| 34.75|null| 11.44| 49.5| 52.24|   0.68|    0.0|  3.27|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-18|67.45058|null|   6.1|25.77| 29.57|null|   6.1|48.43| 53.49|   0.74|   0.21|  2.75|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-19|67.45058|null|  2.51|26.88| 27.45|null|  2.51|50.03| 49.48|   0.26|   0.02|   2.8|null|     Poor|2015|       2.0|\n",
      "|Ahmedabad|2015-01-20|67.45058|null|  7.92| 26.8|  32.4|null|  7.92|58.87| 56.37|   0.24|   0.01|  3.97|null|     Poor|2015|       2.0|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"AQIBucket\", outputCol=\"AQIB_index\").fit(spark_df_filled2)\n",
    "spark_df_ind = indexer.transform(spark_df_filled2)\n",
    "spark_df_ind.show()\n",
    "\n",
    "\n",
    "#df3 = encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "spark_df_ind.createOrReplaceTempView('pollution')\n",
    "\n",
    "results4 = spark.sql(\"SELECT AQIBucket ,AQIB_index, count(AQIB_index), city,year FROM pollution group by AQIBucket, AQIB_index, city,year\")\n",
    "results4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/09 11:01:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, Date, PM2.5, PM10, NO, NO2, NOx, NH3, CO, SO2, O3, Benzene, Toluene, Xylene, AQI, AQI_Bucket\n",
      " Schema: City, Date, PM25, PM10, NO, NO2, NOX, NH3, CO, SO2, O3, benzene, toluene, Xylene, AQI, AQIBucket\n",
      "Expected: PM25 but found: PM2.5\n",
      "CSV file: file:///home/ubuntu/722/aws-722-copy/Datasets/city_day.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+----------+-------------+\n",
      "|     City|      Date|    PM25|PM10|    NO|  NO2|   NOX| NH3|    CO|  SO2|    O3|benzene|toluene|Xylene| AQI|AQIBucket|year|AQIB_index|     AQIB_vec|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+----------+-------------+\n",
      "|Ahmedabad|2015-01-01|67.45058|null|  0.92|18.22| 17.15|null|  0.92|27.64|133.36|    0.0|   0.02|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-02|67.45058|null|  0.97|15.69| 16.46|null|  0.97|24.55| 34.06|   3.68|    5.5|  3.77|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-03|67.45058|null|  17.4| 19.3|  29.7|null|  17.4|29.07|  30.7|    6.8|   16.4|  2.25|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-04|67.45058|null|   1.7|18.48| 17.97|null|   1.7|18.59| 36.08|   4.43|  10.14|   1.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-05|67.45058|null|  22.1|21.42| 37.76|null|  22.1|39.33| 39.31|   7.01|  18.89|  2.78|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-06|67.45058|null| 45.41|38.48|  81.5|null| 45.41|45.76| 46.51|   5.42|  10.83|  1.93|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-07|67.45058|null|112.16|40.62|130.77|null|112.16|32.28| 33.47|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-08|67.45058|null| 80.87|36.74| 96.75|null| 80.87|38.54| 31.89|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-09|67.45058|null| 29.16| 31.0|  48.0|null| 29.16|58.68| 25.75|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-10|67.45058|null|  null| 7.04|   0.0|null|  null| 8.29|  4.55|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-11|67.45058|null|132.07| 55.8| 24.53|null|132.07|25.03|  6.79|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-12|67.45058|null| 52.04|40.67| 90.24|null| 52.04|51.84| 45.89|   2.41|   0.03|  7.88|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-13|67.45058|null| 48.82| 44.2| 87.09|null| 48.82|68.21| 35.16|   9.45|  13.35|  12.5|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-14|67.45058|null|  19.2|27.86| 33.05|null|  19.2|52.65| 20.96|   2.16|   2.26|  5.19|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-15|67.45058|null|   0.6|16.96|  16.6|null|   0.6|28.89| 47.63|   0.14|   0.04|  1.35|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-16|67.45058|null|  1.63|21.72| 22.86|null|  1.63|38.27| 46.03|   0.35|   0.05|  2.01|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-17|67.45058|null| 11.44|24.73| 34.75|null| 11.44| 49.5| 52.24|   0.68|    0.0|  3.27|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-18|67.45058|null|   6.1|25.77| 29.57|null|   6.1|48.43| 53.49|   0.74|   0.21|  2.75|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-19|67.45058|null|  2.51|26.88| 27.45|null|  2.51|50.03| 49.48|   0.26|   0.02|   2.8|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-20|67.45058|null|  7.92| 26.8|  32.4|null|  7.92|58.87| 56.37|   0.24|   0.01|  3.97|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"AQIB_index\", outputCol=\"AQIB_vec\")\n",
    "ohe = encoder.fit(spark_df_ind) # indexer is the existing dataframe, see the question\n",
    "encoded = ohe.transform(spark_df_ind)\n",
    "encoded.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/09 11:01:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, Date, PM2.5, PM10, NO, NO2, NOx, NH3, CO, SO2, O3, Benzene, Toluene, Xylene, AQI, AQI_Bucket\n",
      " Schema: City, Date, PM25, PM10, NO, NO2, NOX, NH3, CO, SO2, O3, benzene, toluene, Xylene, AQI, AQIBucket\n",
      "Expected: PM25 but found: PM2.5\n",
      "CSV file: file:///home/ubuntu/722/aws-722-copy/Datasets/city_day.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOX</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>benzene</th>\n",
       "      <th>toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "      <th>AQIBucket</th>\n",
       "      <th>year</th>\n",
       "      <th>AQIB_index</th>\n",
       "      <th>AQIB_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>67.450577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>18.219999</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>27.639999</td>\n",
       "      <td>133.360001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poor</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>67.450577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>16.459999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>24.549999</td>\n",
       "      <td>34.060001</td>\n",
       "      <td>3.68</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>3.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poor</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>67.450577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.40</td>\n",
       "      <td>19.299999</td>\n",
       "      <td>29.700001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.40</td>\n",
       "      <td>29.070000</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>6.80</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>2.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poor</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>67.450577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>18.480000</td>\n",
       "      <td>17.969999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>18.590000</td>\n",
       "      <td>36.080002</td>\n",
       "      <td>4.43</td>\n",
       "      <td>10.140000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poor</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>67.450577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.10</td>\n",
       "      <td>21.420000</td>\n",
       "      <td>37.759998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.10</td>\n",
       "      <td>39.330002</td>\n",
       "      <td>39.310001</td>\n",
       "      <td>7.01</td>\n",
       "      <td>18.889999</td>\n",
       "      <td>2.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poor</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29526</th>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>2020-06-27</td>\n",
       "      <td>15.020000</td>\n",
       "      <td>50.939999</td>\n",
       "      <td>7.68</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>19.540001</td>\n",
       "      <td>12.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>8.550000</td>\n",
       "      <td>23.299999</td>\n",
       "      <td>2.24</td>\n",
       "      <td>12.070000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>2020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527</th>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>2020-06-28</td>\n",
       "      <td>24.379999</td>\n",
       "      <td>74.089996</td>\n",
       "      <td>3.42</td>\n",
       "      <td>26.059999</td>\n",
       "      <td>16.530001</td>\n",
       "      <td>11.99</td>\n",
       "      <td>0.52</td>\n",
       "      <td>12.720000</td>\n",
       "      <td>30.139999</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>0.38</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29528</th>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>22.910000</td>\n",
       "      <td>65.730003</td>\n",
       "      <td>3.45</td>\n",
       "      <td>29.530001</td>\n",
       "      <td>18.330000</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>8.420000</td>\n",
       "      <td>30.959999</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>68.0</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29529</th>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>16.639999</td>\n",
       "      <td>49.970001</td>\n",
       "      <td>4.05</td>\n",
       "      <td>29.260000</td>\n",
       "      <td>18.799999</td>\n",
       "      <td>10.03</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.840000</td>\n",
       "      <td>28.299999</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29530</th>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>26.850000</td>\n",
       "      <td>14.050000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>17.049999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>2020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29531 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                City        Date       PM25       PM10     NO        NO2  \\\n",
       "0          Ahmedabad  2015-01-01  67.450577        NaN   0.92  18.219999   \n",
       "1          Ahmedabad  2015-01-02  67.450577        NaN   0.97  15.690000   \n",
       "2          Ahmedabad  2015-01-03  67.450577        NaN  17.40  19.299999   \n",
       "3          Ahmedabad  2015-01-04  67.450577        NaN   1.70  18.480000   \n",
       "4          Ahmedabad  2015-01-05  67.450577        NaN  22.10  21.420000   \n",
       "...              ...         ...        ...        ...    ...        ...   \n",
       "29526  Visakhapatnam  2020-06-27  15.020000  50.939999   7.68  25.059999   \n",
       "29527  Visakhapatnam  2020-06-28  24.379999  74.089996   3.42  26.059999   \n",
       "29528  Visakhapatnam  2020-06-29  22.910000  65.730003   3.45  29.530001   \n",
       "29529  Visakhapatnam  2020-06-30  16.639999  49.970001   4.05  29.260000   \n",
       "29530  Visakhapatnam  2020-07-01  15.000000  66.000000   0.40  26.850000   \n",
       "\n",
       "             NOX    NH3     CO        SO2          O3  benzene    toluene  \\\n",
       "0      17.150000    NaN   0.92  27.639999  133.360001     0.00   0.020000   \n",
       "1      16.459999    NaN   0.97  24.549999   34.060001     3.68   5.500000   \n",
       "2      29.700001    NaN  17.40  29.070000   30.700001     6.80  16.400000   \n",
       "3      17.969999    NaN   1.70  18.590000   36.080002     4.43  10.140000   \n",
       "4      37.759998    NaN  22.10  39.330002   39.310001     7.01  18.889999   \n",
       "...          ...    ...    ...        ...         ...      ...        ...   \n",
       "29526  19.540001  12.47   0.47   8.550000   23.299999     2.24  12.070000   \n",
       "29527  16.530001  11.99   0.52  12.720000   30.139999     0.74   2.210000   \n",
       "29528  18.330000  10.71   0.48   8.420000   30.959999     0.01   0.010000   \n",
       "29529  18.799999  10.03   0.52   9.840000   28.299999     0.00   0.000000   \n",
       "29530  14.050000   5.20   0.59   2.100000   17.049999      NaN        NaN   \n",
       "\n",
       "       Xylene   AQI     AQIBucket  year  AQIB_index                   AQIB_vec  \n",
       "0        0.00   NaN          Poor  2015         2.0  (0.0, 0.0, 1.0, 0.0, 0.0)  \n",
       "1        3.77   NaN          Poor  2015         2.0  (0.0, 0.0, 1.0, 0.0, 0.0)  \n",
       "2        2.25   NaN          Poor  2015         2.0  (0.0, 0.0, 1.0, 0.0, 0.0)  \n",
       "3        1.00   NaN          Poor  2015         2.0  (0.0, 0.0, 1.0, 0.0, 0.0)  \n",
       "4        2.78   NaN          Poor  2015         2.0  (0.0, 0.0, 1.0, 0.0, 0.0)  \n",
       "...       ...   ...           ...   ...         ...                        ...  \n",
       "29526    0.73  41.0          Good  2020         4.0  (0.0, 0.0, 0.0, 0.0, 1.0)  \n",
       "29527    0.38  70.0  Satisfactory  2020         1.0  (0.0, 1.0, 0.0, 0.0, 0.0)  \n",
       "29528    0.00  68.0  Satisfactory  2020         1.0  (0.0, 1.0, 0.0, 0.0, 0.0)  \n",
       "29529    0.00  54.0  Satisfactory  2020         1.0  (0.0, 1.0, 0.0, 0.0, 0.0)  \n",
       "29530     NaN  50.0          Good  2020         4.0  (0.0, 0.0, 0.0, 0.0, 1.0)  \n",
       "\n",
       "[29531 rows x 19 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/09 11:01:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, Date, PM2.5, PM10, NO, NO2, NOx, NH3, CO, SO2, O3, Benzene, Toluene, Xylene, AQI, AQI_Bucket\n",
      " Schema: City, Date, PM25, PM10, NO, NO2, NOX, NH3, CO, SO2, O3, benzene, toluene, Xylene, AQI, AQIBucket\n",
      "Expected: PM25 but found: PM2.5\n",
      "CSV file: file:///home/ubuntu/722/aws-722-copy/Datasets/city_day.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+----------+-------------+\n",
      "|     City|      Date|    PM25|PM10|    NO|  NO2|   NOX| NH3|    CO|  SO2|    O3|benzene|toluene|Xylene| AQI|AQIBucket|year|AQIB_index|     AQIB_vec|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+----------+-------------+\n",
      "|Ahmedabad|2015-01-01|67.45058|null|  0.92|18.22| 17.15|null|  0.92|27.64|133.36|    0.0|   0.02|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-02|67.45058|null|  0.97|15.69| 16.46|null|  0.97|24.55| 34.06|   3.68|    5.5|  3.77|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-03|67.45058|null|  17.4| 19.3|  29.7|null|  17.4|29.07|  30.7|    6.8|   16.4|  2.25|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-04|67.45058|null|   1.7|18.48| 17.97|null|   1.7|18.59| 36.08|   4.43|  10.14|   1.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-05|67.45058|null|  22.1|21.42| 37.76|null|  22.1|39.33| 39.31|   7.01|  18.89|  2.78|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-06|67.45058|null| 45.41|38.48|  81.5|null| 45.41|45.76| 46.51|   5.42|  10.83|  1.93|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-07|67.45058|null|112.16|40.62|130.77|null|112.16|32.28| 33.47|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-08|67.45058|null| 80.87|36.74| 96.75|null| 80.87|38.54| 31.89|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-09|67.45058|null| 29.16| 31.0|  48.0|null| 29.16|58.68| 25.75|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-10|67.45058|null|  null| 7.04|   0.0|null|  null| 8.29|  4.55|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-11|67.45058|null|132.07| 55.8| 24.53|null|132.07|25.03|  6.79|    0.0|    0.0|   0.0|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-12|67.45058|null| 52.04|40.67| 90.24|null| 52.04|51.84| 45.89|   2.41|   0.03|  7.88|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-13|67.45058|null| 48.82| 44.2| 87.09|null| 48.82|68.21| 35.16|   9.45|  13.35|  12.5|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-14|67.45058|null|  19.2|27.86| 33.05|null|  19.2|52.65| 20.96|   2.16|   2.26|  5.19|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-15|67.45058|null|   0.6|16.96|  16.6|null|   0.6|28.89| 47.63|   0.14|   0.04|  1.35|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-16|67.45058|null|  1.63|21.72| 22.86|null|  1.63|38.27| 46.03|   0.35|   0.05|  2.01|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-17|67.45058|null| 11.44|24.73| 34.75|null| 11.44| 49.5| 52.24|   0.68|    0.0|  3.27|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-18|67.45058|null|   6.1|25.77| 29.57|null|   6.1|48.43| 53.49|   0.74|   0.21|  2.75|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-19|67.45058|null|  2.51|26.88| 27.45|null|  2.51|50.03| 49.48|   0.26|   0.02|   2.8|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "|Ahmedabad|2015-01-20|67.45058|null|  7.92| 26.8|  32.4|null|  7.92|58.87| 56.37|   0.24|   0.01|  3.97|null|     Poor|2015|       2.0|(5,[2],[1.0])|\n",
      "+---------+----------+--------+----+------+-----+------+----+------+-----+------+-------+-------+------+----+---------+----+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dataframe columns \n",
    "encoded.columns\n",
    "\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/09 11:02:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, Date, PM2.5, AQI_Bucket\n",
      " Schema: City, Date, PM25, AQIBucket\n",
      "Expected: PM25 but found: PM2.5\n",
      "CSV file: file:///home/ubuntu/722/aws-722-copy/Datasets/city_day.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features\n",
       "0  (67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0...\n",
       "1  (67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0...\n",
       "2  (67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0...\n",
       "3  (67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0...\n",
       "4  (67.45057678222656, 2015.0, 2.0, 0.0, 0.0, 1.0..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "inputCols = [\n",
    " 'PM25',\n",
    " 'year',\n",
    " 'AQIB_index',\n",
    " 'AQIB_vec']\n",
    "\n",
    "outputCol = \"features\"\n",
    "df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "encoded = df_va.transform(encoded)\n",
    "encoded.select(['features']).toPandas().head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
